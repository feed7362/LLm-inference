services:
  llama_dev:
    build:
      context: ..
      dockerfile: .devcontainer/Dockerfile
    ports:
      - "2222:2222"
    container_name: llama_container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    volumes:
      - chroma_data:/data/chroma

volumes:
  chroma_data:      