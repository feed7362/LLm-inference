# LLM-Inference

**LLM-Inference** is a FastAPI-based backend for large language model inference over a WebSocket API. Built with LangChain, LangGraph, and llama-cpp-python, it supports OpenAI-compatible local models (e.g., Gemma 3B via llama.cpp) and provides infrastructure for tool-augmented agents including RAG, code execution, and more.

---

## ðŸš€ Features

- ðŸ” **OpenAI-compatible**: Designed for compatibility with OpenAI-style API and tools ecosystem.
- ðŸ§  **Local LLM support**: Optimized for models like Gemma 3B, served through `llama-cpp-python`.
- ðŸ”§ **Tool use via LangChain**: Code execution, calculator, weather, and custom tools (experimental).
- ðŸŒ **WebSocket API**: Real-time inference with client-server communication over WS.
- ðŸ§± **LangGraph agent support**: Supports multistep agent workflows.
- ðŸ³ **Docker-native**: Lightweight multi-stage Docker build reduces final image size.
- âš™ï¸ **Devcontainer support**: Preconfigured for VSCode DevContainers.

---

## ðŸ“¦ Tech Stack

| Component     | Technology           |
|---------------|----------------------|
| API Backend   | FastAPI (async)      |
| Inference     | llama-cpp-python     |
| Agent Logic   | LangChain + LangGraph|
| Web Interface | WebSocket (WS) API   |
| Tool Calling  | LangChain tools      |
| Container     | Docker (multi-stage) |

---

## ðŸ“‚ Project Structure
```plantext
LLm-inference/
â”œâ”€â”€ .devcontainer/         # Dev environment configs
â”œâ”€â”€ agency/                # Agent tools (calculator, RAG, parser, etc.)
â”œâ”€â”€ data/                  # Persistent data (logs, experiments)
â”œâ”€â”€ llama_cpp_chat_model/  # Llama.cpp wrappers and interfaces
â”œâ”€â”€ models/                # Place your language models here
â”œâ”€â”€ test/                  # Test scripts and utility files
â”œâ”€â”€ utils/                 # Config and logging utilities
â”œâ”€â”€ .env.example           # Template for environment variables
â”œâ”€â”€ main.py                # Main entry point
â”œâ”€â”€ manual_launch.py       # Manual runner example
â”œâ”€â”€ llm_manager.py         # Model manager
â”œâ”€â”€ prompts.py             # Prompt templates
â”œâ”€â”€ websocket.py           # Websocket endpoint server
â”œâ”€â”€ Dockerfile, compose-dev.yaml, devcontainer.json
â””â”€â”€ ...
```
---

## âš™ï¸ Setup

### ðŸ“ Clone the repo

```bash
git clone https://github.com/feed7362/llm-inference.git
cd llm-inference
```
### ðŸ³ Build & Run via Docker

```bash
docker build -t llm-inference .
docker run --gpus all -p 2222:2222 llm-inference
```

---

### ðŸ§ª WebSocket Usage

Connect to:
```plaintext
ws://localhost:2222/stream
```

Example message payload (JSON):
```json
{
    "messages": "What's the weather in London now?"
}
```

Response format:
```json
{
  "token": {
    "role": "assistant",
    "content": "The weather in London is partly cloudy with a temperature of 20.4Â°C. The wind is blowing from the southwest at 19.4 km/h, and the relative humidity is 52%. It is daytime.\n"
  }
}
```

---

### ðŸ› ï¸ Tools (Experimental)

The following tools are integrated (statically or dynamically):
- ðŸ§® calculator â€“ mathematical expression evaluation
- ðŸŒ¤ï¸ weather â€“ mock or real-time weather info
- ðŸ“š RAG â€“ retrieval-augmented generation (if vector store is configured)
- ðŸ’» code â€“ code execution (sandboxed)
- ðŸ” search â€“ search engine integration (e.g., Google Custom Search API)

> âš ï¸ These tools are partially tested and may require stability improvements.

---

### ðŸ“ˆ Future Roadmap
âœ… Improve tool testing coverage
- ðŸ” Add rate limiting
- ðŸŒ Add multi-modal support (images, audio)
- ðŸ“Š Integrate observability (e.g., Prometheus/Grafana)
- ðŸ”— Add more LangChain tools
- ðŸ§© Support more LLMs (e.g., Mistral, Qwen, etc.)
- ðŸ”„ Implement CI/CD with GitHub Actions or GitLab

---

### ðŸ§ª Contributing 
Use the following guidelines for contributions:
- Fork the repo and create a feature branch
- Use devcontainer for development with cuda support
- Document your changes and test tools locally
- Submit a pull request with a clear description of changes

> Note: You can use manual_launch.py to run the server without Docker for local testing.
---

### ðŸ“„ License
This project is licensed under the MIT License Â© 2025.

> Note: The Gemma 3B model is developed and distributed by Google DeepMind under its own license terms. Refer to their official model repository for usage and licensing details.