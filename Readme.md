# LLM-Inference

**LLM-Inference** is a FastAPI-based backend for large language model inference over a WebSocket API. Built with LangChain, LangGraph, and llama-cpp-python, it supports OpenAI-compatible local models (e.g., Gemma 3B via llama.cpp) and provides infrastructure for tool-augmented agents including RAG, code execution, and more.

---

## ğŸš€ Features

- ğŸ” **OpenAI-compatible**: Designed for compatibility with OpenAI-style API and tools ecosystem.
- ğŸ§  **Local LLM support**: Optimized for models like Gemma 3B, served through `llama-cpp-python`.
- ğŸ”§ **Tool use via LangChain**: Code execution, calculator, weather, and custom tools (experimental).
- ğŸŒ **WebSocket API**: Real-time inference with client-server communication over WS.
- ğŸ§± **LangGraph agent support**: Supports multi-step agent workflows.
- ğŸ³ **Docker-native**: Lightweight multi-stage Docker build reduces final image size.
- âš™ï¸ **Devcontainer support**: Preconfigured for VSCode DevContainers.

---

## ğŸ“¦ Tech Stack

| Component     | Technology           |
|---------------|----------------------|
| API Backend   | FastAPI (async)      |
| Inference     | llama-cpp-python     |
| Agent Logic   | LangChain + LangGraph|
| Web Interface | WebSocket (WS) API   |
| Tool Calling  | LangChain tools      |
| Container     | Docker (multi-stage) |

---

## ğŸ“‚ Project Structure
```plantext
LLm-inference/
â”œâ”€â”€ .devcontainer/         # Dev environment configs
â”œâ”€â”€ agency/                # Agent tools (calculator, RAG, parser, etc.)
â”œâ”€â”€ data/                  # Persistent data (logs, experiments)
â”œâ”€â”€ llama_cpp_chat_model/  # Llama.cpp wrappers and interfaces
â”œâ”€â”€ models/                # Place your language models here
â”œâ”€â”€ test/                  # Test scripts and utility files
â”œâ”€â”€ utils/                 # Config and logging utilities
â”œâ”€â”€ .env.example           # Template for environment variables
â”œâ”€â”€ main.py                # Main entry point
â”œâ”€â”€ manual_launch.py       # Manual runner example
â”œâ”€â”€ llm_manager.py         # Model manager
â”œâ”€â”€ prompts.py             # Prompt templates
â”œâ”€â”€ websocket.py           # Websocket endpoint server
â”œâ”€â”€ Dockerfile, compose-dev.yaml, devcontainer.json
â””â”€â”€ ...
```
---

## âš™ï¸ Setup

### ğŸ“ Clone the repo

```bash
git clone https://github.com/yourname/llm-inference.git
cd llm-inference
```
### ğŸ³ Build & Run via Docker

```bash
docker build -t llm-inference .
docker run --gpus all -p 2222:2222 llm-inference
```

---

### ğŸ§ª WebSocket Usage

Connect to:
```plaintext
ws://localhost:2222/stream
```

Example message payload (JSON):
```json
{
    "messages": "What's the weather in London now?"
}
```

Response format:
```json
{
  "token": {
    "role": "assistant",
    "content": "The weather in London is partly cloudy with a temperature of 20.4Â°C. The wind is blowing from the southwest at 19.4 km/h, and the relative humidity is 52%. It is daytime.\n"
  }
}
```

---

### ğŸ› ï¸ Tools (Experimental)

The following tools are integrated (statically or dynamically):
- ğŸ§® calculator â€“ mathematical expression evaluation
- ğŸŒ¤ï¸ weather â€“ mock or real-time weather info
- ğŸ“š RAG â€“ retrieval-augmented generation (if vector store is configured)
- ğŸ’» code â€“ code execution (sandboxed)

âš ï¸ These tools are partially tested and may require stability improvements.

---

### ğŸ“ˆ Future Roadmap
âœ… Improve tool testing coverage
- ğŸ” Add rate limiting
- ğŸŒ Add multi-modal support (images, audio)
- ğŸ“Š Integrate observability (e.g., Prometheus/Grafana)
- ğŸ”— Add more LangChain tools
- ğŸ§© Support more LLMs (e.g., Mistral, Qwen, etc.)
- ğŸ”„ Implement CI/CD with GitHub Actions or GitLab

---

### ğŸ§ª Contributing 
Use the following guidelines for contributions:
- Fork the repo and create a feature branch
- Use devcontainer for development with cuda support
- Document your changes and test tools locally
- Submit a pull request with clear description of changes

---

### ğŸ“„ License
MIT License Â© 2025